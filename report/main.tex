% Created 2017-11-30 Thu 13:59
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage[Bjornstrup]{fncychap}
\usepackage[dvipsnames]{xcolor}
\usepackage{minted}
\usepackage[french, english]{babel}
\date{}
\title{}
\hypersetup{
 pdfauthor={Willian Ver Valen Paiva},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.0.50 (Org mode 9.1.2)}, 
 pdflang={English}}
\begin{document}

\begin{titlepage}
\begin{center}

% Upper part of the page. The '~' is needed because only works if a paragraph has started.

\textsc{\LARGE}\\[1.5cm]

\textsc{\Large}\\[0.5cm]

% Title

\vspace{1cm}
\hrule
\vspace{1cm}


{\huge \bfseries Face alignment using deep learning}


\vspace{1cm}
\hrule
\vspace{1cm}


% Author and supervisor
\begin{minipage}{0.4\textwidth}
\emph{Author:} \\
Ver Valem Paiva \textsc{Willian}\\
\end{minipage}

\vfill

% Bottom of the page
{\large \today}

\end{center}
\end{titlepage}


\tableofcontents
\newpage



\section{introduction}
\label{sec:org211dc5a}

For a long time work in facial recognition has being done and one of the key
points of it is face alignment as it poses its own challenge.
And the main tool used for the job is OpenCV which is used with DLIB to
recognize Facial landmarks.

The automatic recognition of landmarks is essential to be able to classify
facial expressions, or face tracking, face animation, and even 3D face
modeling.

For example to classify facial expressions it is necessary to classify
Facial Action Units also known as FACS \cite{ekman1977facial}, which in turn
needs a proper face alignment.

The problem of face alignment is among the most popular in the field of
computer vision and today we have many different implementations to
automatically recognize facial landmarks on images.
The most known is the Active Appearance Model (AAM)
\cite{edwards1998face,matthews2004active}.

But today we also some good results using Deep learning to achieve the
results for example the work done by Adrian Bulat on recognizing 3D facial
landmarks \cite{bulat2017far} that shows remarkable results and is implemented
in \emph{Torch} a framework for \textbf{LUA}. 

As the DLIB model has difficulty on recognizing points on faces by the side
view, this project focus on this problem and does its best to tackle this
problem.


\section{Dataset}
\label{sec:orgdbf23f6}
In this study the MUCT Face Database \cite{Milborrow10} was used
this dataset consists of pictures (resolution 480x640) taken from 276 subjects
using 5 cameras in different angles and light conditions (total of 3755
images) like the image below: 

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/muct-views-lores.jpg}
\caption{There is no images on the left but they can be reproduced by mirroring the right side}
\end{figure}

Each picture is manually coded with 76 facial landmark like:
\begin{center}
\includegraphics[width=.9\linewidth]{./images/landmarks.jpg}
\end{center}


The landmarks are saved in to 4 different file formats

\begin{itemize}
\item muct76.shape       shape file \cite{milborrow2009active}
\item muct76.rda         R data file (www.r-project.org/
\item muct76.csv         comma separated values
\item muct76-opencv.csv  comma separated values in OpenCV coords (0,0 at top left).
\end{itemize}


the coordinate system in these files is the one used by Stasm (i.e. the
origin 0,0 is the center of the image, x increases as you move right, y
increases as you move up).  The exception is muct76-opencv.csv, where the
format is the "OpenCV format" (i.e. the origin 0,0 is at the top left, x
increases as you move left, y increases as you move down). 

Unavailable points are marked with coordinates 0,0 (regardless of the
coordinate system mentioned above).  "Unavailable points" are points
that are obscured by other facial features.  (This refers to landmarks
behind the nose or side of the face -- the position of such landmarks
cannot easily be estimated by human landmarkers -- in contrast, the
position of landmarks behind hair or glasses was estimated by the
landmarkers).  

So any points with the coordinates 0,0 should be ignored.  Unavailable
points appear only in camera views b and c.  

the subjects 247 and 248 are identical twins.

The data set is available via \href{https://github.com/StephenMilborrow/muct}{github} or it is also a submodule on this
project and by recursively initializing or cloning the data can be found in 
folder project/dataset that is the recommended way as the scripts depend on
this folder 


\subsection{Split the training and testing sets}
\label{sec:org1b3e9ff}

As this dataset is composed by multiple images of the same subject the
division of sets for training and testing could not be done at a random way
as an special attention is needed to not have the same subject on the train
and testing set.

And after analyzing it was divided by taken the first 82 subjects for the
testing set (249 images per camera total of 1245 around 30\%)

The function "decompress\(_{\text{dataset}}\)" on the \emph{DataGen.py} takes care of
decompressing the data and splitting the training and testing set. 

\subsection{Data Analyses}
\label{sec:org0a7b42f}

lets start by looking at one image from the database those images are on the
format of 640x480. 
\begin{center}
\includegraphics[width=.9\linewidth]{./images/example1.png}
\end{center}

And lets plot the points on one image to have an insight: 

\begin{center}
\includegraphics[width=.9\linewidth]{./images/example2.png}
\end{center}

As is possible to see those images are large and have big regions with
useless information like the background.

To improve that lets use DLIB's face recognition to find the face area and
crop it, for that we can use the function "crop$\backslash$\(_{\text{image}}\)" from the \emph{DataGen.py} 
this function takes the image and the size to crop we pass just one integer
for the size as it crops the image in a square, and returns the cropped image
and the bound box used to crop the image. 

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/example3.png}
\caption{image cropped with the size of 299 x 299.}
\end{figure}

But as the image have being cropped we also have to move the landmark points
to match the new image, for that we can use the function "replace$\backslash$\(_{\text{landmarks}}\)"
which takes the bound box and the actual landmarks and return the new
landmarks

\begin{center}
\includegraphics[width=.9\linewidth]{./images/example4.png}
\end{center}

the process of cropping the image brings 2 advantages to the project:
\begin{itemize}
\item less resources needed to train model
\item it makes possible to use images with more than one face in the future
\end{itemize}

\subsection{Data Augmentation}
\label{sec:orge7749bf}

As explained before there is no cameras on the left but we can mirror the
right images, starting from that we will implement a some data augmentation 
like flipping every image horizontally and vertically.

To achieve that we can use the function "flip$\backslash$\(_{\text{image}}\)" also from \emph{DataGen.py}
it takes the image, the landmarks, the direction to flip, width and height.
and returns the new image and landmarks.
lets see the result:

\begin{center}
\includegraphics[width=.9\linewidth]{./images/example5.png}
\end{center}

By applying this to all images we finish with a total of 3723 testing images
and 7497 training images  

\section{Models}
\label{sec:org7dbc137}

For this project many different approaches have being taken and it will be
presented here the best outcome of each "kind" of approaches like:

\begin{itemize}
\item A CNN created from scratch
\item Inception transfer learning
\item ResNet50 transfer learning
\end{itemize}

All the models have being trained in a cloud server for performance wise and
saved to later study.

The metrics used to observe the performance of every model was the mean square
error (MSE)

The models code can be found on the file \emph{Models.py}.

The models use a \emph{"npz"} file that can be generated via by using the function :
\begin{verbatim}
DataGen.save_dataset(DataGen.create_dataset(size, True, True), "outputName")
\end{verbatim}

And for the inception transfer learning it uses a bottleneck file that can be
generated with the function:

\begin{verbatim}
DataGen.inception_bottleneck(<npz file>, "outputName")
\end{verbatim}

every model was implemented using early stopping, TensorBoard and saves the
best loss weights on a \emph{hdf5} file   


\pagebreak

\subsection{CNN from scratch}
\label{sec:org73c2e53}

This CNN started with a model based on the AlexNet \cite{krizhevsky2012imagenet} 
\begin{center}
\includegraphics[width=.9\linewidth]{./images/alexnet6.jpg}
\end{center}
But we finished with a model as illustrated below:
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/mymodel.png}
\caption{the full size image can be found on the report/images/mymodel.png.}
\end{figure}

here is the learning curve from this model
\begin{center}
\includegraphics[width=.9\linewidth]{./images/mymodelLoss.png}
\end{center}

as we can see the learning curve for the training set was improving steadily
but for the testing set it starts to get worst  around the epoch 16 and the
early stopping is triggered at epoch 24 as it has no improvement.

by taking the weights of the best loss and doing an inference on a set of
training images we get the following result: 

\begin{center}
\includegraphics[width=.9\linewidth]{./images/mymodelTest.png}
\end{center}

It is noticible that the result is not satisfactory but it has plenty of room
for improvement. but at this point of the project it started to get expensive
to keep using the cloud machine to train and there was yet some other methods
to try out.  

\subsection{Inception transfer learning}
\label{sec:org439d68d}

\subsubsection{bottleneck}
\label{sec:org2909d33}

The next step was to use transfer learning, by using the inceptionV3
\cite{szegedy2016rethinking}, the first approach as to create bottleneck files
from the inception and train a model with those outputs.

As the previous model many models have being made and here is the best result 
found by using this method was the following:

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/btn.png}
\caption{the full size image can be found on the report/images/btn.png.}
\end{figure}

In this model we took a different approach as just using the output of
Inception wasn't given good results 3 different inputs was used the squared
and the \emph{sine} of the inception output was also used in conjunction to the
normal output 

\begin{center}
\includegraphics[width=.9\linewidth]{./images/btnLoss.png}
\end{center} 

The training of this model took 80 epochs before the early stopping was
triggered while the learning curve on the test set is better them the previous
model the loss is slightly bigger them the previous model:

\begin{center}
\includegraphics[width=.9\linewidth]{./images/btnResult.png}
\end{center}

And the results are far from good, worst then the previous model. 

\pagebreak


\subsubsection{Retrain}
\label{sec:orgdbb0adb}

The next step here is to keep using the transfer learning from inception but
give a bit of more room for improvement by training respectively the 2 last
convolutional blocks of the inception and the 4 last ones:

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/incep.png}
\caption{4 block(red) 2 block(black).}
\end{figure}


\pagebreak

\begin{enumerate}
\item \textbf{2 blocks results}
\label{sec:orge4022a3}

\centerline{\includegraphics[height=9cm]{./images/incep249Loss.png}}
\centerline{\includegraphics[height=9cm]{./images/incep249res.png}}

\pagebreak

\item \textbf{4 blocks results}
\label{sec:orgfd2fa35}

\centerline{\includegraphics[height=9cm]{./images/incep197Loss.png}}
\centerline{\includegraphics[height=9cm]{./images/incep197res.png}}


\pagebreak
as we can see it really improved the results by using the 2 first blocks after a
120 epochs we have a steady learning curve and we get a nice prediction on the
testing images, but once we increase the number of training layers by using
the first 4 block, the early stopping triggers 70 epochs before and while
the results look good, however, it is worst then the model with 2 blocks.
\end{enumerate}


\subsection{ResNet50 transfer learning}
\label{sec:orgeadfc91}
Here the same approach that as used with Inception is used with the ResNet50 \cite{he2016deep}  
model, but as the ResNet50 uses images with format 224x224 and we have images
with 299x299, while we could crop the images it is not a good solution as it
was cutting out landmarks from some images. So the idea here is use the
ResNet50 with different input. 

2 models was trained here one that don't train any layer from ResNet50 and
another that trains 2 convolutional blocks like:

\begin{center}
\includegraphics[width=.9\linewidth]{./images/resnet50.jpg}
\end{center}


\pagebreak

\begin{enumerate}
\item \textbf{0 blocks results}
\label{sec:org4ba9ab8}

\centerline{\includegraphics[height=9cm]{./images/resnet0loss.png}}
\centerline{\includegraphics[height=9cm]{./images/resnet0res.png}}


\pagebreak
\item \textbf{2 blocks results}
\label{sec:orge1bf18c}

\centerline{\includegraphics[height=9cm]{./images/resnet2loss.png}}
\centerline{\includegraphics[height=9cm]{./images/resnet2res.png}}

\pagebreak
as it is noticible using just the output of ResNet50 did give better results
them the inception but yet it wasn't satisfactory in the other hand when
retraining 2 block it showed a really promising result
\end{enumerate}

\section{Results}
\label{sec:org13cda33}

after all those tests and trials we can make a loss comparison for each model
like:

\begin{center}
\begin{tabular}{lrr}
model & training loss & validation loss\\
\hline
scratch & 0.01547 & 0.02588\\
inception bottleneck & 0.03599 & 0.03567\\
inception 2 blocks & 0.0001108 & 0.003932\\
inception 4 blocks & 0.0002229 & 0.003802\\
resnet50 0 blocks & 0.01 & 0.013\\
resnet50 2 blocks & 0.00005033 & 0.003779\\
\end{tabular}
\end{center}

by doing this comparison we see that the ResNet50 model has the most
promising results and probably is possible to get even better results by
training more layers or even making some adjustments, but as this kind of
training demands a lot of resources and keep the servers running started to
get out of the budget the tests and development where halted at this point 

\section{Conclusion}
\label{sec:org1687a99}

This project shows that the approach used is correct and that with more
resources is possible to delivery a reliable face alignment. 
The next step on such a project would be by starting by using some other
datasets in conjunction to the MUCT so we can get a better generalization as
all the images on this set has the same background it can bias the result. 

And also by working on the hyper-parameters for the ResNet50 network and
improving the results that would a start for getting a reliable face alignment 

\section{Hardware}
\label{sec:orgbf2996e}

For information propose all the training was executed on a virtual server
hosted on google cloud
The specs are the following:

8 CPUs with 60GB of memory and 1 NVIDIA Tesla P100 for the price of \$1490.51
per month at a rate of 2.042 per hour











\bibliographystyle{unsrt}
\bibliography{repport}
\end{document}